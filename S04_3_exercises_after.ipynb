{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise after class\n",
    "\n",
    "The goal of this exercise is to explore the NLTK library using the manual or online tutorials (avoid AI usage). First, create one (or more) texts in spanish (just copy paste it somewhere). The use the NLTK library of pure Python to performe at least this action to the text(s). \n",
    "\n",
    "1. Case folding\n",
    "2. Word normalization\n",
    "3. Tokenization\n",
    "4. Stemming\n",
    "5. Lemmatization\n",
    "6. Sentence segmentation\n",
    "7. PoS Tagging\n",
    "8. Named Entity Recognition (NER)\n",
    "\n",
    "Just try to explore and understand the library. Check in the reference book and NLTK manual for the new concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "import es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alquiler de coche\n",
      "Buenas tardes, soy Alba. He alquilado un coche para el próximo fin de semana y me gustaría saber dónde debo recogerlo. Yo vivo en Barcelona.\n",
      "Buenas tardes Alba, muchas gracias por llamar. Yo vivo en un pueblo a 20 minutos de Barcelona. ¿Crees que podrías venir a recogerlo el viernes?\n",
      "Depende de a qué hora ya que el viernes trabajo.\n",
      "Me vendría bien entre las 17.00h y las 19.00h. ¿Cómo lo tienes?\n",
      "Lo tengo complicado, acabo de trabajar a las 18.30 y creo que no me daría tiempo. ¿Cómo te iría el sábado a primera hora?\n",
      "Si vinieses antes de las 9.00h me vendría bien.\n",
      "Pues si te parece bien, te aviso cuando salga de Barcelona, sobre las 8.00h, así cuando llegues ya está todo listo.\n",
      "Genial Alba, quedamos así, te espero el sábado antes de las 9.00h.\n"
     ]
    }
   ],
   "source": [
    "with open('S04_3_text.txt', 'r', encoding='utf-8-sig') as file:\n",
    "        text = file.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alquiler de coche\n",
      "buenas tardes, soy alba. he alquilado un coche para el próximo fin de semana y me gustaría saber dónde debo recogerlo. yo vivo en barcelona.\n",
      "buenas tardes alba, muchas gracias por llamar. yo vivo en un pueblo a 20 minutos de barcelona. ¿crees que podrías venir a recogerlo el viernes?\n",
      "depende de a qué hora ya que el viernes trabajo.\n",
      "me vendría bien entre las 17.00h y las 19.00h. ¿cómo lo tienes?\n",
      "lo tengo complicado, acabo de trabajar a las 18.30 y creo que no me daría tiempo. ¿cómo te iría el sábado a primera hora?\n",
      "si vinieses antes de las 9.00h me vendría bien.\n",
      "pues si te parece bien, te aviso cuando salga de barcelona, sobre las 8.00h, así cuando llegues ya está todo listo.\n",
      "genial alba, quedamos así, te espero el sábado antes de las 9.00h.\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "casefolded_text = text.lower()\n",
    "print(casefolded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alquiler de coche buenas tardes  soy alba  he alquilado un coche para el pr ximo fin de semana y me gustar a saber d nde debo recogerlo  yo vivo en barcelona  buenas tardes alba  muchas gracias por llamar  yo vivo en un pueblo a 20 minutos de barcelona   crees que podr as venir a recogerlo el viernes  depende de a qu  hora ya que el viernes trabajo  me vendr a bien entre las 17 00h y las 19 00h   c mo lo tienes  lo tengo complicado  acabo de trabajar a las 18 30 y creo que no me dar a tiempo   c mo te ir a el s bado a primera hora  si vinieses antes de las 9 00h me vendr a bien  pues si te parece bien  te aviso cuando salga de barcelona  sobre las 8 00h  as  cuando llegues ya est  todo listo  genial alba  quedamos as   te espero el s bado antes de las 9 00h \n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "clean_text = re.sub(r'[^a-zA-Z0-9]', ' ', casefolded_text)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alquiler', 'de', 'coche', 'buenas', 'tardes', 'soy', 'alba', 'he', 'alquilado', 'un', 'coche', 'para', 'el', 'pr', 'ximo', 'fin', 'de', 'semana', 'y', 'me', 'gustar', 'a', 'saber', 'd', 'nde', 'debo', 'recogerlo', 'yo', 'vivo', 'en', 'barcelona', 'buenas', 'tardes', 'alba', 'muchas', 'gracias', 'por', 'llamar', 'yo', 'vivo', 'en', 'un', 'pueblo', 'a', '20', 'minutos', 'de', 'barcelona', 'crees', 'que', 'podr', 'as', 'venir', 'a', 'recogerlo', 'el', 'viernes', 'depende', 'de', 'a', 'qu', 'hora', 'ya', 'que', 'el', 'viernes', 'trabajo', 'me', 'vendr', 'a', 'bien', 'entre', 'las', '17', '00h', 'y', 'las', '19', '00h', 'c', 'mo', 'lo', 'tienes', 'lo', 'tengo', 'complicado', 'acabo', 'de', 'trabajar', 'a', 'las', '18', '30', 'y', 'creo', 'que', 'no', 'me', 'dar', 'a', 'tiempo', 'c', 'mo', 'te', 'ir', 'a', 'el', 's', 'bado', 'a', 'primera', 'hora', 'si', 'vinieses', 'antes', 'de', 'las', '9', '00h', 'me', 'vendr', 'a', 'bien', 'pues', 'si', 'te', 'parece', 'bien', 'te', 'aviso', 'cuando', 'salga', 'de', 'barcelona', 'sobre', 'las', '8', '00h', 'as', 'cuando', 'llegues', 'ya', 'est', 'todo', 'listo', 'genial', 'alba', 'quedamos', 'as', 'te', 'espero', 'el', 's', 'bado', 'antes', 'de', 'las', '9', '00h']\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "tokeniced_text = clean_text.split()\n",
    "print(tokeniced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alquiler', 'de', 'coche', 'buenas', 'tardes', 'soy', 'alba', 'he', 'alquilad', 'un', 'coche', 'para', 'e', 'pr', 'xim', 'fin', 'de', 'semana', 'y', 'me', 'gustar', 'a', 'saber', 'd', 'nde', 'deb', 'recoger', 'y', 'viv', 'en', 'barcelona', 'buenas', 'tardes', 'alba', 'muchas', 'gracias', 'por', 'llamar', 'y', 'viv', 'en', 'un', 'pueb', 'a', '20', 'minutos', 'de', 'barcelona', 'crees', 'que', 'podr', 'as', 'venir', 'a', 'recoger', 'e', 'viernes', 'depende', 'de', 'a', 'qu', 'hora', 'ya', 'que', 'e', 'viernes', 'trabaj', 'me', 'vendr', 'a', 'bien', 'entre', 'las', '17', '00h', 'y', 'las', '19', '00h', 'c', 'm', '', 'tienes', '', 'teng', 'complicad', 'acab', 'de', 'trabajar', 'a', 'las', '18', '30', 'y', 'cre', 'que', 'n', 'me', 'dar', 'a', 'tiemp', 'c', 'm', 'te', 'ir', 'a', 'e', 's', 'bad', 'a', 'primera', 'hora', 'si', 'vinieses', 'antes', 'de', 'las', '9', '00h', 'me', 'vendr', 'a', 'bien', 'pues', 'si', 'te', 'parece', 'bien', 'te', 'avis', 'cuand', 'salga', 'de', 'barcelona', 'sobre', 'las', '8', '00h', 'as', 'cuand', 'llegues', 'ya', 'est', 'tod', 'list', 'genia', 'alba', 'quedamos', 'as', 'te', 'esper', 'e', 's', 'bad', 'antes', 'de', 'las', '9', '00h']\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "pure_stemmed = [word.rstrip(\"lo\") for word in tokeniced_text]\n",
    "print(pure_stemmed)\n",
    "#?????????????????????????????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\manue/nltk_data'\n    - 'c:\\\\Users\\\\manue\\\\Anaconda\\\\nltk_data'\n    - 'c:\\\\Users\\\\manue\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\manue\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\manue\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\manue\\Anaconda\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\manue\\Anaconda\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\manue/nltk_data'\n    - 'c:\\\\Users\\\\manue\\\\Anaconda\\\\nltk_data'\n    - 'c:\\\\Users\\\\manue\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\manue\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\manue\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#5\u001b[39;00m\n\u001b[0;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 3\u001b[0m nltk_lemmatizingtext \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokeniced_text]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(nltk_lemmatizingtext)\n",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#5\u001b[39;00m\n\u001b[0;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 3\u001b[0m nltk_lemmatizingtext \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokeniced_text]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(nltk_lemmatizingtext)\n",
      "File \u001b[1;32mc:\\Users\\manue\\Anaconda\\Lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m wn\u001b[38;5;241m.\u001b[39m_morphy(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32mc:\\Users\\manue\\Anaconda\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\manue\\Anaconda\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\manue\\Anaconda\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\manue\\Anaconda\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\manue/nltk_data'\n    - 'c:\\\\Users\\\\manue\\\\Anaconda\\\\nltk_data'\n    - 'c:\\\\Users\\\\manue\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\manue\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\manue\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk_lemmatizingtext = [lemmatizer.lemmatize(word) for word in tokeniced_text]\n",
    "print(nltk_lemmatizingtext)\n",
    "#??????????????????????????????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alquiler de coche\\nBuenas tardes, soy Alba.', 'He alquilado un coche para el próximo fin de semana y me gustaría saber dónde debo recogerlo.', 'Yo vivo en Barcelona.', 'Buenas tardes Alba, muchas gracias por llamar.', 'Yo vivo en un pueblo a 20 minutos de Barcelona.', '¿Crees que podrías venir a recogerlo el viernes?', 'Depende de a qué hora ya que el viernes trabajo.', 'Me vendría bien entre las 17.00h y las 19.00h.', '¿Cómo lo tienes?', 'Lo tengo complicado, acabo de trabajar a las 18.30 y creo que no me daría tiempo.', '¿Cómo te iría el sábado a primera hora?', 'Si vinieses antes de las 9.00h me vendría bien.', 'Pues si te parece bien, te aviso cuando salga de Barcelona, sobre las 8.00h, así cuando llegues ya está todo listo.', 'Genial Alba, quedamos así, te espero el sábado antes de las 9.00h.']\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "pure_sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "print(pure_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alquiler', 'PROPN'), ('de', 'ADP'), ('coche', 'NOUN'), ('\\n', 'SPACE'), ('Buenas', 'PROPN'), ('tardes', 'NOUN'), (',', 'PUNCT'), ('soy', 'AUX'), ('Alba', 'PROPN'), ('.', 'PUNCT'), ('He', 'AUX'), ('alquilado', 'VERB'), ('un', 'DET'), ('coche', 'NOUN'), ('para', 'ADP'), ('el', 'DET'), ('próximo', 'ADJ'), ('fin', 'NOUN'), ('de', 'ADP'), ('semana', 'NOUN'), ('y', 'CCONJ'), ('me', 'PRON'), ('gustaría', 'VERB'), ('saber', 'AUX'), ('dónde', 'PRON'), ('debo', 'AUX'), ('recogerlo', 'VERB'), ('.', 'PUNCT'), ('Yo', 'PRON'), ('vivo', 'VERB'), ('en', 'ADP'), ('Barcelona', 'PROPN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('Buenas', 'PROPN'), ('tardes', 'NOUN'), ('Alba', 'PROPN'), (',', 'PUNCT'), ('muchas', 'PRON'), ('gracias', 'NOUN'), ('por', 'ADP'), ('llamar', 'VERB'), ('.', 'PUNCT'), ('Yo', 'PRON'), ('vivo', 'VERB'), ('en', 'ADP'), ('un', 'DET'), ('pueblo', 'NOUN'), ('a', 'ADP'), ('20', 'NUM'), ('minutos', 'NOUN'), ('de', 'ADP'), ('Barcelona', 'PROPN'), ('.', 'PUNCT'), ('¿', 'PUNCT'), ('Crees', 'PROPN'), ('que', 'SCONJ'), ('podrías', 'NOUN'), ('venir', 'VERB'), ('a', 'ADP'), ('recogerlo', 'VERB'), ('el', 'DET'), ('viernes', 'NOUN'), ('?', 'PUNCT'), ('\\n', 'SPACE'), ('Depende', 'VERB'), ('de', 'ADP'), ('a', 'ADP'), ('qué', 'PRON'), ('hora', 'NOUN'), ('ya', 'ADV'), ('que', 'SCONJ'), ('el', 'DET'), ('viernes', 'NOUN'), ('trabajo', 'ADJ'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('Me', 'PRON'), ('vendría', 'VERB'), ('bien', 'ADV'), ('entre', 'ADP'), ('las', 'DET'), ('17.00h', 'NUM'), ('y', 'CCONJ'), ('las', 'DET'), ('19.00h', 'NUM'), ('.', 'PUNCT'), ('¿', 'PUNCT'), ('Cómo', 'PRON'), ('lo', 'PRON'), ('tienes', 'VERB'), ('?', 'PUNCT'), ('\\n', 'SPACE'), ('Lo', 'PRON'), ('tengo', 'VERB'), ('complicado', 'ADJ'), (',', 'PUNCT'), ('acabo', 'VERB'), ('de', 'ADP'), ('trabajar', 'VERB'), ('a', 'ADP'), ('las', 'DET'), ('18.30', 'NOUN'), ('y', 'CCONJ'), ('creo', 'VERB'), ('que', 'SCONJ'), ('no', 'ADV'), ('me', 'PRON'), ('daría', 'VERB'), ('tiempo', 'NOUN'), ('.', 'PUNCT'), ('¿', 'PUNCT'), ('Cómo', 'PRON'), ('te', 'PRON'), ('iría', 'VERB'), ('el', 'DET'), ('sábado', 'NOUN'), ('a', 'ADP'), ('primera', 'ADJ'), ('hora', 'NOUN'), ('?', 'PUNCT'), ('\\n', 'SPACE'), ('Si', 'SCONJ'), ('vinieses', 'NOUN'), ('antes', 'ADV'), ('de', 'ADP'), ('las', 'DET'), ('9.00h', 'NOUN'), ('me', 'PRON'), ('vendría', 'VERB'), ('bien', 'ADV'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('Pues', 'NOUN'), ('si', 'SCONJ'), ('te', 'PRON'), ('parece', 'VERB'), ('bien', 'ADV'), (',', 'PUNCT'), ('te', 'PRON'), ('aviso', 'VERB'), ('cuando', 'SCONJ'), ('salga', 'VERB'), ('de', 'ADP'), ('Barcelona', 'PROPN'), (',', 'PUNCT'), ('sobre', 'ADP'), ('las', 'DET'), ('8.00h', 'PROPN'), (',', 'PUNCT'), ('así', 'ADV'), ('cuando', 'SCONJ'), ('llegues', 'PROPN'), ('ya', 'ADV'), ('está', 'AUX'), ('todo', 'PRON'), ('listo', 'ADJ'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('Genial', 'PROPN'), ('Alba', 'PROPN'), (',', 'PUNCT'), ('quedamos', 'VERB'), ('así', 'ADV'), (',', 'PUNCT'), ('te', 'PRON'), ('espero', 'VERB'), ('el', 'DET'), ('sábado', 'NOUN'), ('antes', 'ADV'), ('de', 'ADP'), ('las', 'DET'), ('9.00h', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "nlp = es_core_news_sm.load()\n",
    "doc = nlp(text)\n",
    "print([(w.text, w.pos_) for w in doc]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
